import os
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from concurrent.futures import ThreadPoolExecutor

# Base URL of the folder
folders = ['A&W1', 'A&W2', 'A&W3', 'Harbour1', 'Harbour2', 'Harbour3',
           'Harbour4', 'SFU_art', 'blue_ceiling', 'dining_area',
           'downtown_smith','edu_area', 'foodcourt_mcnz', 'hallway',
           'image_theater', 'owl_statue','playground', 'rugs','seat_rows',
           'study_area','stump','subway1','subway2','theater','tree_tunel',
           'uncle_faith1','uncle_faith2','under_tree2','wall_art',
           'wall_hallway','wall_lab']
#folder = 'edu_area' # change the folder you want to download

for folder in folders:
    base_url = 'https://www2.cs.sfu.ca/~colour/data2/DRONE-Dataset/scenes_shots/' + folder + '/'

    # Send a GET request to the URL
    response = requests.get(base_url)

    # Parse the content using BeautifulSoup
    soup = BeautifulSoup(response.text, 'html.parser')

    # Find all anchor tags (<a>) that contain file links
    file_links = soup.find_all('a', href=True)

    # Create a directory to save files
    os.makedirs(folder, exist_ok=True)

    def download_file(link):
        # Get the file URL
        file_url = urljoin(base_url, link['href'])

        # Check if it's a file (not a subdirectory)
        if not file_url.endswith('/') and file_url.lower().endswith('.nef'):
            file_name = os.path.join(folder, link['href'])
            try:
                # Send a GET request to download the file
                file_response = requests.get(file_url, timeout=30)

                # Save the file to the directory if the request was successful
                if file_response.status_code == 200:
                    with open(file_name, 'wb') as file:
                        file.write(file_response.content)
                    print(f"Downloaded: {file_name}")
                else:
                    print(f"Failed to download: {file_url} (Status code: {file_response.status_code})")

            except requests.RequestException as e:
                print(f"Error downloading {file_url}: {e}")

    # Download files in parallel using ThreadPoolExecutor
    max_workers = 32
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        executor.map(download_file, file_links)

    print("All files have been downloaded. Folder: {folder}")
